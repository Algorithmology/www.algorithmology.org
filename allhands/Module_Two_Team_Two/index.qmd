---
author: [Molly Suppo, Daniel Bekele, Add Name Here, Add Name here, Add Name Here]
title: "Investigating Test Priotitization in Traditional Sorting Algorithms vs. a Multi Objective Sorting Algorithm"
page-layout: full
categories: [post, sorting, comparison]
date: "2025-03-27"
date-format: long
toc: true
---

# Repository Link

Below is the link that will direct you to our GitHub repository needed to run our experiment on your personal device: 
<https://github.com/suppo01/Algorithm-Analysis-All-Hands-Module-2>

## Introduction

During our team's exploration of sorting algorithms and their performance characteristics, an interesting research question emerged: How can we effectively sort test cases considering multiple factors simultaneously? Traditional sorting algorithms excel at sorting based on a single criterion, but real-world test case prioritization often requires balancing multiple objectives, such as execution time and code coverage.

This research question led us to investigate the potential of multi-objective optimization algorithms, specifically NSGA-II (Non-dominated Sorting Genetic Algorithm II), in comparison to traditional sorting approaches. While traditional algorithms like Quick Sort and Bubble Sort can sort test cases based on one factor at a time, NSGA-II offers the advantage of considering multiple objectives simultaneously, potentially providing more nuanced and practical test case prioritization.

Our research aims to answer the question: How does NSGA-II compare to traditional sorting algorithms in terms of running time when comparing test cases in terms of execution speed and code coverage factors? The traditional algorithms would sort according to one factor at a time while NSGA-II would sort according to both factors.

### Data Collection and Gathering

For our research, we selected the Chasten project, a publicly available GitHub repository developed as part of a course in our department. This choice was strategic for several reasons:

1. **Reproducibility**: Since Chasten was developed within our department, we have direct access to its development history and can ensure that others can replicate our study.

2. **Test Infrastructure**: The project includes a comprehensive test suite, making it an ideal candidate for analyzing test case execution times and coverage metrics.

3. **Tool Development**: As a tool developed in an academic setting, Chasten provides a controlled environment for our research, with well-defined test cases and clear execution patterns.

To collect our data, we developed a custom script (`collect_test_metrics.py`) that leverages `pytest-cov`, a pytest plugin for measuring code coverage. The script executes the test suite using Poetry's task runner with specific `pytest-cov` configurations to generate detailed coverage reports. The coverage data is collected using the following command structure:

```bash
pytest --cov=chasten tests/ --cov-report=json
```

This command generates a `coverage.json` file that contains detailed coverage information for each module in the project. The JSON structure includes:
- Executed lines for each file
- Summary statistics including covered lines, total statements, and coverage percentages
- Missing and excluded lines
- Context information for each covered line

The coverage.json file is structured hierarchically, with each module's data organized under the "files" key. For example:
```json
{
    "files": {
        "chasten/checks.py": {
            "executed_lines": [...],
            "summary": {
                "covered_lines": 50,
                "num_statements": 51,
                "percent_covered": 98,
                "missing_lines": 1,
                "excluded_lines": 0
            }
        }
    }
}
```

To prepare this data for analysis, we developed a mapping script (`mapper.py`) that serves two key purposes:

1. **Traditional Analysis Format**: The script reads both the coverage.json and test_metrics.json files, creating a mapping between test cases and their corresponding module coverage data. It computes a ratio of covered lines to test duration for each test case, which helps identify tests that provide the best coverage per unit of time.

2. **NSGA-II Format**: The script also transforms the data into a specialized format required by the NSGA-II algorithm. Each test case is represented as a list of `[test name, duration, coverage]`, where coverage is the raw number of covered lines from the coverage.json file. This format enables multi-objective optimization, allowing us to simultaneously consider both test execution time and code coverage.

The mapping process ensures proper handling of edge cases:
- Failed or skipped tests are assigned zero coverage values
- Tests with zero duration are handled gracefully
- Each test case is correctly associated with its corresponding module's coverage data
- The data is structured appropriately for both traditional and multi-objective analysis approaches

This data preparation pipeline enables us to:
- Compare the effectiveness of different test prioritization approaches
- Analyze the trade-offs between test execution time and coverage
- Generate reproducible results for both traditional and NSGA-II algorithms
- Maintain data consistency across different analysis methods

## Implementation

The implementation of this project required the use of serveral algorithms. As we were comparing traditional algorithms with
multi objective algorithms. We decided upon two different traditional algorithms, quick sort and bubble sort. As for the multi
objective algorithms, we had the NSGA-II sorting algorithm recommended to us by Professor Kaphammer, so we decided to look into
that one. More specifics about each algorithm are below.

### The Quick Sort Algorithm

### The Bubble Sort Algorithm

### The NSGA-II Multi Objective Algorithm

The NGSA-II multi objective sorting algorithm is broken down into a variety of approaches. We utilized the binary tournament
approach and slightly adapted it to suite our needs. The file that runs this part of the experiment has two main parts, the
`binary_tournament` function and `main`. 

The `binary_tournament` function runs the bulk of the experiment, utilizing a list of indices that indicate the opponents for
each tournament to be performed, `P`, and the population object storing all the objects to be pitted against each other in
tournaments, `pop`. From there, the tournaments are run continuously until all of them have been completed. In the
implementation, the function also collects and constantly updates the list of names dictating the winners with a list also
dictated for the losers to help update the list of winners. At the end, the final winner's list is printed. It is worth noting
that there are slightly different outcomes each time. This could be due to slightly different evaluations occurring each time
as there are serveral aspects that go into running the algorithm, even with a limited number of factors to consider. It is also
worth noting that the variable `S` refers to the result returned by the function, a list of the memory locations for all the
winners. As that is not as helpful to our purposes, it is not seen in our results.

``` python
for i in range(n_tournaments):
        a, b = P[i]

        # if the first individual is better, choose it
        if pop[a].F < pop[b].F:
            S[i] = a
            loser = pop[b].name
            winner = pop[a].name
        # otherwise take the other individual
        else:
            S[i] = b
            loser = pop[a].name
            winner = pop[b].name
   
        # update lists with name records
        if winner not in winner_list:
            if winner not in loser_list:
                winner_list.append(winner)
            else:
                winner_list.remove(loser)
        if loser not in loser_list:
            loser_list.append(loser)

    # return the names of the ideal tests
    print(f"The Ideal Tests Are: {winner_list}")
    return S
```

Main, on the other hand, looks into generating the list of competitor indices using the nested for loop method as that allowed
the result to be made as a list of lists instead of a list of tuples which is not the right format for the `binary_tournament`
function. Also, main generates the Population object. First, a 2d numpy array is created from the JSON file designated for use
by the NSGA-II algorithm as the formatting is slightly different to accomodate the `binary_tournament` function. Then, a list
of Individual objects is created from the information in the array. Finally, that list is passed into a brand new Population
object. Finally, main runs the tournaments by calling the `binary_tournament` function with the Population object and array of
competitor index pairs passed in.

## The Results

## Conclusion
