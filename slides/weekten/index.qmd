---
title: "Recursion and Dynamic Programming"
echo: true
description: "How do you efficiently repeat operations?"
date: "2025-03-17"
date-format: long
author: Gregory M. Kapfhammer
execute:
  freeze: auto
format:
  live-revealjs:
    completion: true
    theme: default
    css: ../css/styles.css
    history: false
    scrollable: true
    transition: slide
    highlight-style: github
    html-math-method: mathjax
    footer: "Algorithmology"
---

# Recap prior topics

::: {.incremental style="margin-top: -0.25em; font-size: 0.9em;"}

- Programming in Python
- Object-oriented analysis and design
- Software testing and debugging
- Algorithm analysis and complexity
- Design and conduct timing experiments
- Abstract data types and data structures

:::

::: {.fragment .fade .boxed-content style="margin-top: -0.05em; font-size: 0.9em;"}

**Goals**: make wise decisions about correctness and performance when
designing, implementing, maintaining, and revising programs

:::

## Learning objectives

::: {.incremental style="margin-top: -0.25em; font-size: 0.8em;"}

- {{< iconify fa6-solid circle-check >}} **CS-202-1**: Correctly
implement both well-established and custom data structures using a programming
language so as to solve a problem with a computer program.

- {{< iconify fa6-solid circle-check >}} **CS-202-2**: Perform an asymptotic
analysis of an algorithm to arrive at its correct worst-case time complexity
class.

- {{< iconify fa6-solid circle-check >}} **CS-202-3**: Conduct experiments that
measure the efficiency of different combinations of programming languages, data
structures, and algorithms.

- {{< iconify fa6-solid circle-check >}} **CS-202-4**: Use both theoretical and
experimental results to pick the data structure(s) and algorithm(s) that
balance the trade-offs associated with correctly and efficiently solving a
problem with a computer program.

:::

::: {.fragment .fade .boxed-content style="margin-top: -0.05em; font-size: 0.9em;"}

{{< iconify fa6-solid rocket >}} *Hooray*, we've worked hard to achieve each of
these objectives!

:::

## There is another learning objective! And, we've not covered it fully!

::: {.fragment .fade .boxed-content style="margin-top: -0.05em; font-size: 0.85em;"}

- **CS-202-5**: Effectively apply algorithmic problem solving techniques like
searching, sorting, and memoization to correctly and efficiently solve a problem
through the use of a computer program.

:::

::: {.incremental style="margin-top: -0.05em; font-size: 0.9em;"}

- Thorough investigation of basic linear searching
- Brief investigation of sorting through a project
- We need to study algorithmic strategies further!
- Investigate **recursive functions** and **dynamic programming**
- Unlocks a new class of **algorithms** and **data structures**

:::

# Recursion means a "reference to self"

- Seen with `SingleLinkedList` and `DoubleLinkedList`!
- Implementation of recursion in Python language
- Mental model for how to think about recursive functions
- Use of recursion as a problem solving technique
- Analysis of the running time of recursive functions

## Recursive implementation of `sumk`

```{python}
def sumk(k):
    if k > 0:
        return sumk(k-1) + k
    return 0

print(sumk(1))
print(sumk(2))
print(sumk(3))
print(sumk(4))
print(sumk(5))
```

::: {.fragment style="font-size: 0.975em; margin-top: -0.15em"}

- The `sumk` function calls itself with a smaller value of `k`
- The base case stops the recursion when `k` is equal to `0`

:::

## Try out the `sumk` function

```{pyodide}
#| autorun: true
#| max-lines: 10
def sumk(k):
    if k > 0:
        return sumk(k-1) + k
    return 0

print(sumk(1))
print(sumk(2))
print(sumk(3))
print(sumk(4))
print(sumk(5))
```

::: fragment

{{< iconify fa6-solid draw-polygon >}} What picture best explains the `sumk`
function's behavior?

:::

## Termination of recursive functions

::: fragment

{{< iconify fa6-solid gears >}} **How to ensure that a recursive function stops running?**

  - **Base case** stops the recursion by returning fixed value
  - **Recursive case** reduces the input towards the base case
  - **Recursive calls** are made of smaller sub-problems

:::

::: {.fragment style="font-size: 1em; margin-top: -0.25em"}

{{< iconify fa6-solid gears >}} **What happens if the base case is not reached?**

  - The recursive function will enter an infinite recursion
  - Python limits adding recursive functions to call stack 
  - The program will ultimately raise the `RecursionError`

:::

## Calling the `sumsquarek` function

```{python}
def sumsquarek(k):
    if k == 0:
        return 0
    else:
        return k ** 2 + sumsquarek(k - 1)

print(sumsquarek(1))  # Output: 1
print(sumsquarek(2))  # Output: 5
print(sumsquarek(3))  # Output: 14
print(sumsquarek(4))  # Output: 30
print(sumsquarek(5))  # Output: 55
```

::: {.fragment style="font-size: 0.9em; margin-top: -0.25em"}

- The `sumsquarek` function calls itself with a smaller value of `k`
- The base case stops the recursion when `k` is equal to `0`

:::

## Try out the `sumsquarek` function

```{pyodide}
#| autorun: true
#| max-lines: 10
def sumsquarek(k):
    if k == 0:
        return 0
    else:
        return k ** 2 + sumsquarek(k - 1)

print(sumsquarek(1))
print(sumsquarek(2))
print(sumsquarek(3))
print(sumsquarek(4))
print(sumsquarek(5))
```

::: fragment

{{< iconify fa6-solid gear >}} Can you crash the `sumsquarek` function?
Why does it crash?

:::

## Recursion and the call stack

::: {.fragment .fade .boxed-content style="margin-top: -0.05em; font-size: 0.95em;"}

- **Call stack** stores information about active functions
- **Stack frame** stores information about a function call
- **Stack overflow** occurs when the call stack is full
- **RecursionError** is raised when the call stack is full

:::

::: {.fragment .fade style="margin-top: -0.05 font-size: 0.95em;"}

- {{< iconify fa6-solid gears >}} **Behavior of a recursive function**:
    - **Push** a new stack frame when a function is **called**
    - **Pop** the stack frame when the function **returns**
    - **Continue** until the **base case** is reached or **stack overflow**

:::

## `RecursionError` with functions

```python
def a(k):
    if k == 0: return 0
    return b(k)

def b(k):
    return c(k)

def c(k):
    return a(k-1)

a(340)
```

::: {.fragment style="font-size: 1em; margin-top: 0.25em"}

- This program will raise a `RecursionError` in Python
- It does not contain a (direct) recursive function!
- Error signals that Python reached limit of call stack

:::

## Create your own `RecursionError`!

```{pyodide}
#| autorun: false
#| max-lines: 10
def a(k):
    if k == 0: return 0
    return b(k)

def b(k):
    return c(k)

def c(k):
    return a(k-1)

a(340)
```

::: {.fragment style="font-size: 1em; margin-top: 0.05em"}

- This program will raise a `RecursionError` in Python
- Error reported with a diagnostic message
- Can you explain why this program crashes?

:::

## `RecursionError` with `list`s

```python
A = [2]
B = [2]
A.append(A)
B.append(B)
A == B
```

::: {.fragment style="font-size: 1em; margin-top: 0.25em"}

- `list.__eq__` method that compares two lists for `==` use
- The first elements of `A` and `B` are equal
- The second elements of `A` and `B` are actually lists
- This causes another call to the `list.__eq__` method
- Ultimately, this leads to a `RecursionError` in Python
- Recursion is elegant but can lead to unexpected errors!

:::

## Iterative and recursive Fibonacci

```{python}
def fibonacci_recursive(k):
    if k in [0,1]: return k
    return fibonacci_recursive(k-1) + fibonacci_recursive(k-2)

print([fibonacci_recursive(i) for i in range(10)])
```

<p class="codespacer">

```{python}
def fibonacci_iterative(k):
    a, b = 0,1
    for i in range(k):
        a, b = b, a + b
    return a

print([fibonacci_iterative(i) for i in range(10)])
```

::: {.fragment style="font-size: 1em; margin-top: -0.2em"}

- Both approaches compute the Fibonacci sequence. Which one do you think is
faster? Why do you think that is the case?

:::

## Use `fibonacci_recursive` and `fibonacci_iterative`

```{pyodide}
#| autorun: true
#| max-lines: 12
def fibonacci_recursive(k):
    if k in [0,1]: return k
    return fibonacci_recursive(k-1) + fibonacci_recursive(k-2)

def fibonacci_iterative(k):
    a, b = 0,1
    for i in range(k):
        a, b = b, a + b
    return a

print("Recursive: " + str([fibonacci_recursive(i) for i in range(10)]))
print("Iterative: " + str([fibonacci_iterative(i) for i in range(10)]))
```

## Recursive greatest common divisor

```{python}
def gcd(a, b):
    if a == b:
        return a
    if a > b:
        a, b = b, a
    return gcd(a, b - a)

print("GCD of 12 and 513 is", gcd(12, 513))
print("GCD of 19 and 513 is", gcd(19, 513))
print("GCD of 19 and 515 is", gcd(515, 19))
```

::: {.fragment style="font-size: 1.05em; margin-top: 0.25em"}

- Computes the greatest common divisor of two numbers
- Too many recursive calls when `b` is much larger than `a`!

:::

## Revising the recursive `gcd`

```{python}
def gcd_revised(a, b):
    if a > b:
        a, b = b, a
    if a == 0:
        return b
    return gcd_revised(a, b % a)

print("GCD of 12 and 513 is", gcd_revised(12, 513))
print("GCD of 19 and 513 is", gcd_revised(19, 513))
print("GCD of 19 and 515 is", gcd_revised(515, 19))
```

::: {.fragment style="font-size: 1em; margin-top: 0.25em"}

- Both approach compute the same sequence of values
- Depending on inputs, one approach may be more efficient

:::

## Compare iteration and recursion

::: {.fragment style="font-size: 0.9em" .callout-note icon=true title="Empirically comparing the performance of iterative and recursive functions"}

1. **Implement both versions of the algorithm**
   - Create a recursive implementation that uses function calls
   - Create an iterative implementation using loops

2. **Create benchmarking harness**
   - Use `time.perf_counter()` for precise measurement
   - Implement a function like `timetrials()` that runs multiple trials

3. **Design controlled experiments**
   - Use a doubling experiment with appropriate input sizes
   - Generate consistent test data for both implementations

4. **Collect and analyze data**
   - Record execution times for each implementation
   - Calculate doubling ratios to determine time complexity

:::

# What is dynamic programming?

::: {.fragment .boxed-content style="font-size: 1em;"}

- Solve a problem using solutions to sub-problems
- Often starts with an inefficient recursive algorithm
- Memoization stores the results of expensive function calls
- Tabulation stores the results of a bottom-up computation

:::

## What is a change-making algorithm?

::: {.fragment .boxed-content style="font-size: 1em;"}

- **Given**: a list of coin denominations and a target amount
- **Find**: minimum number of coins to make target amount

:::

::: fragment

- **Arrive at a suitable solution with the following approach**:
    - Technique that only works for "canonical" coin systems
    - Recursive algorithm that works but is slow sometimes
    - Memoized recursive algorithm that is faster than recursive
    - Dynamic programming algorithm that is efficient
    - Optimized approaches for specific coin systems

:::

## "Defective" Greedy Change-Making

```{python}
def greedyMC(coinvalueList, change):
    coinvalueList.sort()
    coinvalueList.reverse()
    numcoins = 0
    for c in coinvalueList:
        numcoins += change // c
        change = change % c
    return numcoins

print(greedyMC([1, 5, 10, 25], 63)) # Correct answer of 6
print(greedyMC([1, 21, 25], 63))    # Incorrect, should be 3
print(greedyMC([1, 5, 21, 25], 63)) # Incorrect, should be 3
```

::: {.fragment style="font-size: 0.9em; margin-top: 0.1em"}

- What is the minimum number of coins to make change for `63` cents? Wait, this
`greedyMC` only works for "canonical" coin systems!

:::

## Slow Recursive Change-Making

```python
def recMC(coinValueList, change):
   minCoins = change
   if change in coinValueList:
     return 1
   else:
      for i in [c for c in coinValueList if c <= change]:
         numCoins = 1 + recMC(coinValueList,change-i)
         if numCoins < minCoins:
            minCoins = numCoins
   return minCoins

print(recMC([1, 5, 10, 25], 63))
print(recMC([1, 21, 25], 63))
print(recMC([1, 5, 21, 25], 63))
```

::: fragment

- `recMC` calls itself with a smaller value of `change`
- Works correctly --- but is very slow for certain input values!

:::

## Memoized Recursive Change-Making

```{python}
def memoMC(coinValueList, change, knownResults):
    minCoins = change
    if change in coinValueList:
        knownResults[change] = 1
        return 1
    elif change in knownResults:
        return knownResults[change]
    else:
        for i in [c for c in coinValueList if c <= change]:
            numCoins = 1 + memoMC(coinValueList, change-i, knownResults)
            if numCoins < minCoins:
                minCoins = numCoins
                knownResults[change] = minCoins
    return minCoins

knownresults = {}
print(f"{memoMC([1, 5, 10, 21, 25], 63, knownresults)} coins needed.", end=" ")
print(f"Wow, computed {len(knownresults)} intermediate results!", end=" ")
```

## Using dynamic programming

```python
def dpMakeChange(coinValueList, change):
    minCoins = [None]*(change + 1)  # <1>
    for cents in range(change+1):   # <2>
        minCoins[cents] = cents     # <3>
        for c in coinValueList:     # <4>
            if cents >= c:          # <4>
                minCoins[cents] = min(minCoins[cents], minCoins[cents - c] + 1) # <4>
    return minCoins[change]         # <5>

print(dpMakeChange([1,5,10,21,25], 63))
```
1. Create a list to store the answers to the sub-problems
2. For values from `0` to `change`, compute min number of coins
3. Assume at first that all `1` coins are used in solution
4. Determine if different coins can better make change
5. Return the element of the table with best solution

## Dynamic programming

::: {.fragment style="font-size: .9em; margin-top: 0.1em"}

#### {{< iconify fa6-solid gears >}} Key strategy in the `dpMakeChange` function

- Avoid recursive calls and memoization dictionary
- Starting with small values, build up the dictionary
- This is the essence of dynamic programming!

:::

::: {.fragment style="font-size: .9em; margin-top: .1em"}

#### {{< iconify fa6-solid bolt >}} Implementation details of the `dpMakeChange` function

- Use a list accessed by an integer index
- Determine the next best step by trying each coin
- Subtract coin value from current value
- Check list for number of coins needed for remaining change

:::

## Change-making time complexities

::: {.fragment style="font-size: 0.8em" .callout-note icon=true title="What is
the worst-case time complexity of a change-making algorithm?"}

- **Greedy algorithm**: `greedyMC`
  - **Time complexity**: $O(m)$ where $m$ is number of coin types
  - Correct only for "canonical" coin systems like US coins

- **Recursive algorithm**: `recMC`
  - **Time complexity**: $O(c^n)$ where $c$ is a constant and $n$ is the change amount
  - Correct for all coin systems but exponentially slow

- **Memoized recursive algorithm**: `memoMC`
  - **Time complexity**: $O(n \times m)$ where $n$ is change amount and $m$ is number of coin types
  - Stores results of subproblems in a dictionary
  - Correct for all coin systems and much faster than naive recursion

- **Dynamic programming algorithm**: `dpMakeChange`
  - **Time complexity**: $O(n \times m)$ where $n$ is change amount and $m$ is number of coin types
  - Builds solution bottom-up using an array
  - Avoids recursion overhead and dictionary lookups

:::

## What are the similarities and differences between memoization and dynamic programming?

::: {.columns}

::: {.column style="font-size: 0.9em;"}

- {{< iconify fa6-solid arrow-down-wide-short >}} **Recursion**

  - Works top down
  - Start with largest problem
  - Breaks to smaller problem

:::

::: {.column style="font-size: 0.9em;"}

- {{< iconify fa6-solid arrow-up-wide-short >}} **Dynamic Programming**
    - Works bottom up
    - Starts with smallest problem
    - Builds up to larger problem

:::
:::

::: {.fragment .fade .boxed-content style="margin-top: 0.1em; font-size: 0.9em;"}

Both algorithmic strategies can compute correct answer! Compare by
**experimentally studying runtime** and **analyzing running time**.

:::

## Longest common subsequence

```{python}
def lcs_recursive(X, Y):
    if X == "" or Y == "":
        return ""
    if X[-1] == Y[-1]:
        return lcs_recursive(X[:-1], Y[:-1]) + X[-1]
    else:
        return max([lcs_recursive(X[:-1], Y),
                    lcs_recursive(X, Y[:-1])], key = len)

lcs_str = lcs_recursive('ABCBDAB', 'BDCAB')
lcs_len = len(lcs_str)
print(f"LCS length is {lcs_len} and LCS contents are {lcs_str}")
```

::: {.fragment style="font-size: 0.875em; margin-top: -0.1em"}

- Code runs practically forever on moderate-sized inputs
- No matches in long strings yield depth-`n` binary call tree
- Wow, this means that there are $2^n$ recursive calls!

:::

## Try out `lcs_recursive`

```{pyodide}
#| autorun: true
#| max-lines: 20
def lcs_recursive(X, Y):
    if X == "" or Y == "":
        return ""
    if X[-1] == Y[-1]:
        return lcs_recursive(X[:-1], Y[:-1]) + X[-1]
    else:
        return max([lcs_recursive(X[:-1], Y),
                    lcs_recursive(X, Y[:-1])], key = len)

seq1 = "ACGTACGT"; seq2 = "AAGGTCGT"
lcs_dna = lcs_recursive(seq1, seq2)
print(f"Longest Common Subsequence: {lcs_dna} (length: {len(lcs_dna)})")

word1 = "ALGORITHM"; word2 = "ALTRUISTIC"
lcs_word = lcs_recursive(word1, word2)
print(f"Longest Common Subsequence: {lcs_word} (length: {len(lcs_dna)})")
```

## LCS with dynamic programming

```{python}
def lcs_dynamic(X, Y):
    t = {}
    for i in range(len(X)+1): t[(i,0)] = ""
    for j in range(len(Y)+1): t[(0,j)] = ""

    for i, x in enumerate(X):
        for j, y in enumerate(Y):
            if x == y:
                t[(i+1,j+1)] = t[(i, j)] + x
            else:
                t[(i+1,j+1)] = max([t[(i, j+1)], t[(i+1, j)]], key = len)
    return t[(len(X), len(Y))]

lcs_str = lcs_dynamic('ABCBDAB', 'BDCAB')
lcs_len = len(lcs_str)
print(f"LCS length is {lcs_len} and LCS contents are {lcs_str}")
```

::: {.fragment style="font-size: 0.875em; margin-top: -0.1em"}

- Total running time is $O(k \times (m \times n))$ where $k$ is output length

:::

## Try out `lcs_dynamic`

```{pyodide}
#| autorun: true
#| max-lines: 16
def lcs_dynamic(X, Y):
    t = {}
    for i in range(len(X)+1): t[(i,0)] = ""
    for j in range(len(Y)+1): t[(0,j)] = ""

    for i, x in enumerate(X):
        for j, y in enumerate(Y):
            if x == y:
                t[(i+1,j+1)] = t[(i, j)] + x
            else:
                t[(i+1,j+1)] = max([t[(i, j+1)], t[(i+1, j)]], key = len)
    return t[(len(X), len(Y))]

seq1 = "ACGTACGT"; seq2 = "AAGGTCGT"
lcs_dna = lcs_recursive(seq1, seq2)
print(f"Longest Common Subsequence: {lcs_dna} (length: {len(lcs_dna)})")

word1 = "ALGORITHM"; word2 = "ALTRUISTIC"
lcs_word = lcs_recursive(word1, word2)
print(f"Longest Common Subsequence: {lcs_word} (length: {len(lcs_dna)})")
```

## Only calculating the length of LCS

```{python}
def lcs_calculate(X, Y):
    m = len(X)
    n = len(Y)
    L = [[0] * (n + 1) for _ in range(m + 1)]
    for i in range(m + 1):
        for j in range(n + 1):
            if i == 0 or j == 0:
                L[i][j] = 0
            elif X[i - 1] == Y[j - 1]:
                L[i][j] = L[i - 1][j - 1] + 1
            else:
                L[i][j] = max(L[i - 1][j], L[i][j - 1])
    return L[m][n]

lcs_len = lcs_calculate('ABCBDAB', 'BDCAB')
print(f"LCS length is {lcs_len}")
```

::: {.fragment style="font-size: 0.875em; margin-top: -0.1em"}

- `lcs_calculate` function is $O(m \times n)$, but does not return the LCS!

:::

## Algorithmic problem solving

::: {.columns}

::: {.column style="font-size: 0.85em;"}

::: fragment

### Recursion

#### Implementation

- Problem solved by recursion
- Function calls itself
- Reduces problem size

#### Performance

- Risk of stack overflow
- Can be inefficient
- Optimized by memoization

:::

:::

::: {.column style="font-size: 0.85em;"}

::: fragment

### Dynamic Programming

#### Implementation

- Solves complex problems
- Stores subproblem results
- Uses table to store results

#### Performance

- Efficient for subproblem repeats
- Polynomial time complexity
- Space complexity concerns

:::

:::

:::
