---
title: "Running Time Analysis through Analytical Evaluation"
echo: true
description: "How do you use proofs to measure the performance of a Python program?"
date: "2024-02-19"
date-format: long
author: Gregory M. Kapfhammer
execute:
  freeze: auto
format:
  revealjs:
    theme: default
    css: styles.css
    monofont: Ubuntu Mono
    history: false
    scrollable: true
    transition: slide
    highlight-style: github
    html-math-method: mathjax
    footer: "Algorithmology"
---

# Programmers must write programs that are *correct*, *efficient*, and *maintainable*

<!-- Note: the source code in this slide deck demonstrates that Quarto leverages
previously defined functions in later-defined code blocks. This means that source
code on later slides can assume existence of functions in prior slides! -->

::: fragment

- **Prior Focus**: is the program implemented correctly?
- **New Focus**: does the program have good performance?

:::

## A two-week road map for exploring Python program performance

::: {.fragment .fade-right style="margin-top: 0.25em; font-size: 0.95em;"}

- Week 1: **Empirical Evaluation of Runtime**

    - How do you conduct timing experiments to measure the performance of a
    real Python programs?

- Week 2: **Analytical Evaluation of Time Complexity**

    - How do you use an instruction cost model to prove that it belongs to a
    specific time complexity class?

:::

::: {.fragment .fade .boxed-content style="margin-top: -0.25em; font-size: 0.9em;"}

**Ultimate Goal**: create a nuanced description of program efficiency that
adapts to *different inputs* and to *different computers*

:::

## Algorithm analysis challenges 

::: {.columns style="margin-top: 1.25em;"}

::: {.fragment .column .fade style="margin-top: 0.25em;"}

### Different Inputs

- Varying sizes of input data
- Different types of data
- Degrees of sortedness
- Degrees of duplication
- Different data distributions

:::

::: {.fragment .column style="margin-top: 0.25em;"}

### Different Computers

- Diverse hardware setups
- Varying system performance
- Diverse system building
- Different operating systems
- Variation in system load

:::

:::

::: {.fragment .fade .boxed-content style="margin-top: -0.25em; font-size: 0.9em;"}

**Ultimate goal**: experimental and analytical evaluation methods that yield
actionable insights that support *understanding* and *prediction*

:::

## Review the "naive" duplicate detector

```{python}
from typing import List
def duplicates(input_list: List[int]) -> bool:
    n = len(input_list)
    for i in range(n):
        for j in range(n):
            if i != j and input_list[i] == input_list[j]:
                return True
    return False

assert(duplicates([1,2,6,3,4,5,6,7,8]))
assert(not duplicates([1,2,3,4]))
print(duplicates([1,2,6,3,4,5,6,7,8]))
print(not duplicates([1,2,3,4]))
```

::: {.fragment .fade-left style="margin-top: -0.25em; font-size: 0.9em;"}

- Function contains a double-nested `for` loop
- How do we measure the performance of `duplicates`?

:::

## Review the `timetrials` function

::: {style="font-size: 0.925em;"}

```{python}
from typing import Callable
import time

def timetrials(function: Callable, n: int, trials: int = 10) -> None:
    """Time the function with n inputs trials times"""
    totaltime = 0
    for _ in range(trials):
        start = time.time()
        function(list(range(n)))
        totaltime += time.time() - start
    print("average =%10.7f for n = %d" % (totaltime/trials, n))

# conduct a doubling experiment for a provided function
for n in [50, 100, 200, 400, 800, 1600, 3200]:
    timetrials(duplicates, n)
```

:::

## Performance of `duplicates`

```{python}
# calculate a doubling ratio
# for the last two performance
# values when run on the
# instructor computer; note
# that this will vary from
# the displayed values slightly
ratio = (0.2225006 / 0.0541614)
print(f"ratio = {ratio:.4f}")
```

::: {.fragment .fade style="margin-top: -0.25em; font-size: 0.9em;"}

- Repeatedly double the size of the input when running experiments
- A doubling ratio is the current time divided by the prior time
- A ratio of approximately `4` reveals a quadrupling of performance
- Offers experimental evidence that performance is *quadratic*!
- What would a ratio of about `2` or `8` reveal about performance?

:::

# Connection between the empirical and the analytical evaluation of algorithm performance

::: {.fragment .fade style="margin-top: -0.05em; font-size: 0.95em;"}

- **Runtime**: conduct experiments to evaluate performance
- **Running Time**: program structure to characterize performance

:::

## Function to sum the first `k` integers

```{python}
from typing import Tuple
def sumk(k: int) -> Tuple[int, float]:
    start = time.time()
    total = 0
    for i in range(k+1):
        total = total + i
    end = time.time()
    return total, (end - start)

print(sumk(5))
print(sumk(10))
print(sumk(15))
print(sumk(20))
print(sumk(25))
```

## Function to sum the first `k` integers

```{python}
def timetrials_int(func, k, trials = 10):
    totaltime = 0
    for _ in range(trials):
        totaltime += func(k)[1]
    print("average =%10.7f for k = %d" % (totaltime/trials, k))

timetrials_int(sumk, 10000)
timetrials_int(sumk, 100000)
timetrials_int(sumk, 1000000)
timetrials_int(sumk, 10000000)
```

::: {.fragment .fade style="margin-top: -0.2em; font-size: 0.9em;"}

- The `sumk` function outputs the sum of the first `k` integers
- This function has a `for` loop that iterates `k` times
- Ratio `0.203677/0.0220115 = 9.25` reveals *linear* performance

:::

## What do we know about the performance of the `sumk` function?

::: incremental

- As `k` goes up by a factor of `10`, the `timetrials` function shows the `sumk`
runtime also goes up by a factor of `10`

- `sumk` has to do about `k` additions and assignments

- Or, the runtime of `sumk` is **proportional to** `k`

- Find what runtime is proportional to, **not** the **exact time**

- Relationship **should hold** regardless of the computer used!

- Wait, $\sum_{i = 1}^k i = 1 + 2 + 3 + \cdots + k = k (k + 1) / 2$

:::

## Fast summing for the first `k` integers

```{python}
def sumk_calc(k):
    start = time.time()
    total = (k*(k+1)//2)
    end = time.time()
    return total, end-start

timetrials_int(sumk_calc, 10000)
timetrials_int(sumk_calc, 100000)
timetrials_int(sumk_calc, 1000000)
timetrials_int(sumk_calc, 10000000)
timetrials_int(sumk_calc, 100000000)
```

::: {.fragment .fade style="margin-top: -0.2em; font-size: 0.9em;"}

- Wow, the `sumk_calc` function is *much* faster than `sumk`!
- Direct calculation is faster than using `for` loop in `sumk`!

:::

## Modeling program running time

```{python}
def f001(k):
    return [sum([i, i + 1] * 100) for i in range(k)]

print(f001(9))
```

::: {.fragment .fade style="font-size: 0.95em;"}

- Synthetic function `f001` contains many Python constructs
- Remember, lines of code by itself does not signal performance
- Instead, focus on **counts** of the number of **atomic operations**
- Contextualize the operation count based on input size
- **Faster functions** require **fewer atomic operations**
- Remember, as the input grows, the code slows!

:::

## Wait, what is an atomic operation?

::: {.incremental .fade style="margin-top: -0.5em; font-size: 0.975em;"}

- Atomic operations include:
    - Arithmetic and boolean operations
    - Variable assignment
    - Accessing the value of a variable from its name
    - Branching (jumping to another part of the code for `if`/`for`/`while` statements)
    - Calling a function
    - Returning from a function
- Atomic operations are roughly equivalent in required work

:::

## Atomic operations for `list` functions

::: {.fragment .fade style="margin-top: -0.5em; font-size: 0.725em;"}

| Operation Name          | Code      | Cost |
|-------------------------|-----------|:------:|
| index access            | `L[i]` | $1$ |
| index assignment        | `L[i] = newvalue` |	$1$ |
| Append                  | `L.append(newitem)`	| $1$ |
| Pop (from end of list)  | `L.pop()` | $1$ |
| Pop (from index `i`)    | `L.pop(i)`	| $n - i$ |
| Insert at index `i`     | `insert(i, newitem)` |	$n-i$ |
| Delete an item (at index `i`) | `del(item)` |	$n - i$ |
| Membership testing      | `item in L` | $n$ |
| Slice                   | `L[a:b]` | $b-a$
| Concatenate two lists   | `L1 + L2` | $n_1 + n_2$ |
| Sort                    | `L.sort()` | $n \log_2 n$ |

:::

## What do we know about the performance of `List` operations?

::: incremental

- Table reports the asymptotic cost of `List` operations

- Understand which operations produce a new list

- Concatenation and slicing both produce a new collection

- Grasp the running time by intuition of work completed

- `List` and `str` have similar performance characteristics

:::

::: {.fragment .fade .boxed-content style="margin-top: -0.25em; font-size: 0.9em;"}

**Key Insight**: counting the maximum number of atomic operations for a
function's input size yields its worst-case running time

:::
